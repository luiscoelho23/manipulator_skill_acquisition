import torch
import torch.optim as optim
import torch.nn as nn
import numpy as np
import gym
import DmpObstacleEnv
from ActorCriticNN import Actor, Critic
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class A2CAgent:
    def __init__(self, state_dim, action_dim, hidden_dim=256, lr=3e-4, gamma=0.99):
        self.actor = Actor(state_dim, action_dim, hidden_dim)
        self.critic = Critic(state_dim, hidden_dim)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        self.gamma = gamma

    def choose_action(self, state):
        state = torch.FloatTensor(state)
        mean, std = self.actor(state)
        action_distribution = torch.distributions.Normal(mean, std)
        action = action_distribution.sample()
        action_clamped = action.clamp(-0.1, 0.1)  # Ensure action is within the environment's action space
        log_prob = action_distribution.log_prob(action_clamped).sum()
        return action_clamped.detach().numpy(), log_prob
    
    def update(self, trajectory):
        states, actions, rewards, next_states, dones, log_probs = zip(*trajectory)
        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        log_probs = torch.stack(log_probs)

        # Normalize rewards
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)

        # Compute targets for critic
        values = self.critic(states).squeeze()
        next_values = self.critic(next_states).squeeze()
        returns = rewards + self.gamma * next_values
        advantages = returns - values

        # Critic loss
        critic_loss = advantages.pow(2).mean()

        # Actor loss with entropy regularization
        actor_loss = (-log_probs * (advantages.detach()+ 1e-8)).mean()

        # Update actor and critic networks
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        self.actor_loss = actor_loss
        self.critic_loss = critic_loss

    def train(self, env, max_episodes):
        for episode in range(max_episodes):
            state = env.reset()
            trajectory = []
            total_reward = 0
            while 1:
                action, log_prob = self.choose_action(state)
                next_state, reward, done = env.step(action)
                trajectory.append((state, action, reward, next_state, done, log_prob))
                state = next_state
                total_reward += reward

                if done:
                    break

            self.update(trajectory)
            print(f"Episode {episode + 1} | Total Reward: {total_reward} | ActorLoss: {self.actor_loss} | CriticLoss: {self.critic_loss}")

if __name__ == "__main__":

    env = DmpObstacleEnv.DmpObstacleEnv()
    gamma = 0.99
    learning_rate = 3e-4
    hidden_dim = 256
    state_dim = 7
    action_dim = 3

    agent = A2CAgent(state_dim, action_dim, hidden_dim, learning_rate, gamma)

    agent.train(env, max_episodes = 10000)

""" 
def gaussian_noise(action, mean=0.0, std_dev=0.01):
    noise = np.random.normal(mean, std_dev, size=action.shape)
    noisy_action = action + noise
    return noisy_action 

# Training function
def train(actor, critic, env, episodes):
    for episode in range(episodes):
        state = env.reset()
        values = []
        rewards = []
        done = False

        while not done:

            state = torch.FloatTensor(state).unsqueeze(0).to(device)
            action = actor(state)
            action = gaussian_noise(action.detach().cpu().numpy()[0])
            value = critic(state)

            state, reward, done = env.step(action)
            values.append(value)
            rewards.append(reward)

            if done:
                # Compute the returns
                returns = []
                R = 0
                for r in reversed(rewards):
                    R = r + gamma * R
                    returns.insert(0, R)
                returns = torch.FloatTensor(returns).to(device)

                # Compute advantage
                values = torch.cat(values).squeeze()  # Ensure values are of the correct shape
                advantage = returns - values

                # Update actor
                actor_loss = advantage.mean()
                actor_optimizer.zero_grad()
                actor_loss.backward(retain_graph=True)
                actor_optimizer.step()

                # Update critic
                critic_loss = advantage.pow(2).mean()
                critic_optimizer.zero_grad()
                critic_loss.backward()
                critic_optimizer.step()

                print(f"Episode {episode}, Reward: {sum(rewards)}")

def save_models(actor, critic):
    save_path = "saved_models"
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    torch.save(actor.state_dict(), os.path.join(save_path, "actor.pth"))
    torch.save(critic.state_dict(), os.path.join(save_path, "critic.pth"))
    print(f"Models saved to {save_path}")

# Train the model
train(actor, critic, env, episodes=50)

save_models(actor, critic)

# Close the environment
env.close() """